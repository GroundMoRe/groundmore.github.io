<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>
<link href="https://fonts.googleapis.com/css2?family=Open+Sans&display=swap" rel="stylesheet">
<link rel="stylesheet" type="text/css" href="./utils/style_project_page.css" media="screen" />
<link href="https://fonts.googleapis.com/css?family=Arvo|Roboto&display=swap" rel="stylesheet">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
<link rel="stylesheet" href="https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css">
<link rel="stylesheet" href="https://unpkg.com/@glidejs/glide/dist/css/glide.core.min.css">


<html lang="en">

<head>
    <title>Motion-Grounded Video Reasoning: Understanding and Perceiving Motion at Pixel Level</title>

    <meta charset="UTF-8">

    <!-- Add your Google Analytics tag here -->
    <!-- <script async
            src=""></script> -->
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() {
            dataLayer.push(arguments);
        }
        gtag('js', new Date());
        gtag('config', 'UA-97476543-1');
    </script>

</head>

<body>
    <div class="container">
        <h1 class="project-title">
            Motion-Grounded Video Reasoning:<br> Understanding and Perceiving Motion at Pixel Level
        </h1>

        <!-- <div class="conference">
            Arxiv
        </div> -->

        <br><br>

        <!-- Andong Deng1
, Tongjia Chen2
, Shoubin Yu3
, Wenshuo Chen4
,
Taojiannan Yang5
, Lincoln Spencer1
, Erhang Zhang4
, Yapeng Tian6
,
Ajmal Saeed Mian2
, Mohit Bansal3
, Chen Chen1 -->
        <div class="authors">
            <a href=https://dengandong.github.io>
                Andong Deng <sup>1</sup>
            </a>
            <a href=https://tomchen-ctj.github.io>
                Tongjia Chen <sup>2</sup>
            </a>
            <a href=https://yui010206.github.io>
                Shoubin Yu <sup>3</sup>
            </a>
            <a href=https://github.com/shurdy123>
                Wenshuo Chen <sup>4</sup>
            </a> <br>
            <a href=https://taoyang1122.github.io>
                Taojiannan Yang <sup>5</sup>
            </a>
            <a>
                Lincoln Spencer <sup>1</sup>
            </a>
            <a href=https://github.com/zhangerhang>
                Erhang Zhang <sup>4</sup>
            </a>
            <a href=https://www.yapengtian.com>
                Yapeng Tian <sup>6</sup>
            </a> <br>
            <a href=https://ajmalsaeed.net>
                Ajmal Saeed Mian <sup>2</sup>
            </a>
            <a href=https://www.cs.unc.edu/~mbansal>
                Mohit Bansal <sup>3</sup>
            </a>
            <a href=https://www.crcv.ucf.edu/chenchen>
                Chen Chen <sup>1</sup>
            </a>
        </div>
        <br>

        <div class="affiliations">
            <span><sup>1</sup> Center for Research in Computer Vision, University of Central Florida</span> <br>
            <span><sup>2</sup> University of Western Australia</span>
            <span><sup>3</sup> UNC, Chapel Hill </span>
            <span><sup>4</sup> Shandong University </span> <br>
            <span><sup>5</sup> Amazon Web Services </span>
            <span><sup>6</sup> University of Texas at Dallas </span>
        </div>
        <br><br>

        <div class="project-icons">
            <a href="">
                <i class="fa fa-file-pdf-o"></i> <br />
                Paper(coming soon)
            </a>
            <a href="">
                <i class="fa fa-github"></i> <br />
                Code(coming soon) <br />
            </a>
            <a href="https://huggingface.co/datasets/dengandong/GroundMoRe">
                <img src="https://huggingface.co/front/assets/huggingface_logo-noborder.svg" alt="Hugging Face" style="width:24px;height:24px;"> <br />
                Dataset <br />
            </a>
            <!-- <a href="https://github.com/tomchen-ctj/OST">
            <i class="fa fa-youtube-play"></i> <br/>
            Video (in preparation)
        </a> -->
        </div>

        <div class="teaser">
            <br>
            <p style="width: 90%; text-align: left;">
                In this work, we introduce a new task, <b>Motion-Grounded Video Reasoning</b>, designed to assess
                multimodal
                models' reasoning and perception capabilities for motion understanding. We collect a large-scale and
                versatile video dataset, named <b>GroundMoRe</b> for the proposed
                <b>Motion-Grounded Video Reasoning</b> task. We further propose a simple baseline model, <b>MoRA</b>,
                which achieves SOTA performance on <b>GroundMoRe</b>.
            </p>
            <br>
            <br>
            <img src="./images/teaser.png" alt="Teaser figure." style="display:block; margin:auto; width:100%;" />

            <p style="width: 90%; text-align: left;">
                The illustration of the comparison between our <b>Motion-Grounded Video Reasoning</b> and previous video
                motion understanding tasks.
                (a) <i>Action Recognition</i> predicts motion classes for curated video clips; (b) <i>Temporal Action
                    Localization</i> distinguishes action temporal boundaries based on snippet-level features; (c)
                <i>Motion Expression Video Segmentation</i> leverages referring expressions to segment target objects
                but lacks implicit reasoning ability; (d) <i>Spatiotemporal Action Detection</i> predicts both
                spatiotemporal tubes and action labels while only highlighting human.
                Existing video motion understanding tasks (a)-(d) could at most address one or two key problems, either
                lacking fine-grained spatiotemporal perception or ignoring motion-related reasoning.
                (e) Our Motion-Grounded Video Reasoning considers both subject and object in motion as well as
                temporally adjacent events, performing challenging reasoning given four types of questions
                (<b><i>Causal, Sequential, Counterfactual, and Descriptive</i></b>) carefully designed in our
                <b>GroundMoRe</b> dataset and output <b>spatiotemporal masks</b> to indicate the answer visually at the
                <b>pixel level</b>.
                For instance, in the question <i>"who needs to be passed or else the main in grey cannot easily
                    score?"</i>, the motion <i>"pass"</i> and the subject <i>"the man in grey"</i> as well as an
                adjacent event <i>"easily score"</i> are provided in this question, the model needs reason about the
                object <i>"the man in pink pants"</i>, while output spatiotemporal masks (only between 0 to 32s
                where the motion <i>"pass"</i> happens).
                Such a paradigm fully grasps the spatiotemporal contexts of motion and provides an explainable response
                to evaluate the motion understanding ability. The colors of the questions are corresponded to the
                spatiotemporal masks.
            </p>
        </div>

        <br><br>

        <hr>
        <h1>Abstract</h1>

        <p style="width: 85%">
            In this paper, we introduce <b>Motion-Grounded Video Reasoning</b>, a new motion understanding task
            that requires generating spatiotemporal segmentation masks according to the input question, and hence
            needs implicit spatiotemporal reasoning and grounding. This task extends existing spatiotemporal
            grounding work that focuses on explicit action/motion recognition, to a more general format by enabling
            implicit motion reasoning via questions.
            To facilitate the development of advanced motion-grounding models on such a task, we collect a
            large-scale dataset called <b>GroundMoRe</b>, which comprises 1,673 video clips, 243K object masks
            that are deliberately designed with 4 types (<i>Causal, Sequential, Counterfactual, and
                Descriptive</i>) for benchmarking deep and comprehensive motion understanding abilities.
            Our <b>GroundMoRe</b> uniquely requires models to generate visual answers (spatiotemporal masks), providing
            a
            more concrete and visually interpretable response than plain text.
            It evaluates models on spatiotemporal grounding and reasoning, helping address complex challenges in
            video reasoning, temporal perception, and pixel-level understanding. To further facilitate the proposed
            task, we propose a baseline model, <b>Mo</b>tion-Grounded Video <b>R</b>easoning <b>A</b>ssistant
            (<b>model</b>). \model incorporates the multimodal reasoning ability from Multimodal LLM and the
            pixel-level perception capability from the grounding model (SAM) as well as an additional temporal
            localization head. \model achieves respectable performance on <b>GroundMoRe</b> outperforming the best
            existing
            visual grounding baseline model by an average of 28.8\% relatively, but there still remains substantial
            room for interesting future improvements by the community.
            We hope this novel and challenging task will pave the way for future advancements in robust and general
            motion understanding via video reasoning segmentation. </p>
        <br>
        </p>

        <hr>
        <h1>Overview of our <b>GroundMoRe</b></h1>
        <img src="./images/case_vis.png" alt="Visualization." style="display:block; margin:auto; width:80%;" />
        <p style="width: 85%">
        Visualizations of our proposed <b>GroundMoRe</b>. As shown, our <b>GroundMoRe</b> requires
        advanced motion reasoning abilities in diverse scenarios. As illustrated in the fourth row of the figure, the
        question <i>What might not be held by the man if it had not been unwrapped from the paper?</i> requires the
        model to reason the wrapping relationship between <i>the man</i>, <i>the paper</i> and <i>the piston</i> as well
        as the causal connections in the challenging <i>counterfactual</i> setting. Additionally,
        we can observe from the case in the seventh row that our <b>GroundMoRe</b> includes spatiotemporal grounding
        context as well as motion-related attributes understanding. The answer to the question <i>Who might not have
            fallen into the blue cushion on the wall if he had not tripped while trying to defend?</i> can only be
        determined at the end of the video clip. For the question <i>Who is the more offensive player?</i>, the model
        must infer motion-based implicit attributes from the video sequence, demonstrating a strong need for world-level
        commonsense reasoning ability.</p>
        <br>
        <br>
        <h1><b>GroundMoRe</b> Statistics</h1>
        <div class="subfigure-container">
            <div class="subfigure">
                <img src="./images/QS_type_dist.png" alt="Visualization.">
                <p style="text-align: center">Question and Scene Type Distribution of <b>GroundMoRe</b>.</p>
            </div>
            <div class="subfigure">
                <img src="./images/wordcloud.png" alt="Visualization.">
                <p style="text-align: center">Word cloud of the top 100 words in the question annotation in our <b>GroundMoRe</b> dataset.</p>
            </div>
        </div>
        <div class="subfigure-container">
            <div class="subfigure">
                <img src="./images/verb_dist.png" alt="Visualization.">
                <p style="text-align: center">Verb distribution of the motion concepts in <b>GroundMoRe</b>.</p>
            </div>
            <div class="subfigure">
                <img src="./images/obj_dist.png" alt="Visualization.">
                <p style="text-align: center">Object distribution of <b>GroundMoRe</b>.</p>
            </div>
        </div>
        <div class="subfigure-container">
            <div class="subfigure">
                <img src="./images/more_statistics.png" alt="Visualization.">
                <p style="text-align: center">More statistics of <b>GroundMoRe</b>.</p>
            </div>
            <div class="subfigure">
                <img src="./images/Sankey.png" alt="Visualization.">
                <p style="text-align: center">Sankey diagram on the interaction triplets of our <b>GroundMoRe</b>.</p>
            </div>
        </div>
        <br>
        <br>

        <h1>Overview of our proposed baseline <b>MoRA</b></h1>
        <img src="./images/pipeline.png" style="width: 85%" alt="Pipeline." /></br>
        <p style="width: 85%">
            An overview of our proposed baseline <b>MoRA</b>. <b>MoRA</b> adopts the spatiotemporal pooling strategy and
            inserts the extra special <b>[SEG]</b> token.
            Additionally, to enable the temporal localization ability, <b>MoRA</b> takes advantage of the extra
            <b>[LOC]</b>
            token to learn a binary temporal mask, which refines the direct SAM outputs.
        <br>
        <br>
        <h1>Quantitative Results</h1>
        <p style="width: 85%">
            <b>Motion-Grounded Video Reasoning</b> results on our <b>GroundMoRe</b>. We compare all
methods in a zero-shot setting. We bold the best numbers, and underlined the second-best numbers.</p></br>
        <img src="./images/quant_mgvr.png" style="width: 85%" alt="Zero-shot" />
        </p>
        <br>

        <hr>
        <h1>BibTex</h1>

        <!-- <div class="paper-info">
            <span style="font-size: 14pt; font-weight: bold;">Motion-Grounded Video Reasoning:
                Understanding and Perceiving Motion at Pixel Level</span><br>
            <span style="font-size: 14pt;"> Andong Deng, Tongjia Chen, Shoubin Yu, Wenshuo Chen, Taojiannan Yang, Lincoln Spencer, Erhang Zhang, Yapeng Tian, Ajmal Saeed Mian, Mohit Bansal, Chen Chen.
            </span> <br>
            <span style="font-size: 14pt;">Arxiv</span> -->

            <pre><code>@article{
    groundmore,
    title={Motion-Grounded Video Reasoning: Understanding and Perceiving Motion at Pixel Level},
    author={Andong Deng, Tongjia Chen, Shoubin Yu, Wenshuo Chen, Taojiannan Yang, Lincoln Spencer, Erhang Zhang, Yapeng Tian, Ajmal Saeed Mian, Mohit Bansal, Chen Chen.},
    booktitle={arxiv preprint},
    year={2024}, 
 }
</code></pre>
        </div>

        <br><br>


        <br><br>
        <hr>
        <!-- <h1>Acknowledgements</h1> -->

        <p style="width: 85%;">
            The webpage template is adapted from
            <a href="https://qianlim.github.io/POP">POP</a>.

        </p>

        <br><br>
    </div>

</body>

</html>
